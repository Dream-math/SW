<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

# TIHU Open Deep Learning Compiler Stack  
_Based on tvm_  

Apache TVM is a compiler stack for deep learning systems. It is designed to close the gap between the
productivity-focused deep learning frameworks, and the performance- and efficiency-focused hardware backends.
TVM works with deep learning frameworks to provide end to end compilation to different backends.

TIHU compiler stack integrates the nvdla compiler and runtime into tvm, and necessary modifications are made to
improve the efficiency of hardware computing.

# What we have done

<div align=center>
<img src="../doc/compiler_structure.png" width="600" height="400" alt="TIHU"/><br/>
</div>

The compiler consists of the parser, optimizer, codegen, operator libraries, runtime, driver.  
* Parser: Model parsing, which converts deep learning framework models into compiler-defined intermediate graph representations.  
* Optimizer: Graph optimization, supports constant folding, operator fusion, and other optimizations.  
* Codegen: Generates code that can run on TIHU based on the optimization graph.  
* Runtime: Runtime, running computing tasks, including device and memory management, task queue generation, execution control, device status API, model library files, data saving, and loading, image preprocessing, and result analysis.  
* Driver: xdma driver.  

Based on TVM [BYOC](https://tvm.apache.org/docs/dev/how_to/relay_bring_your_own_codegen.html) we integrate our backend into tvm, as the above shows,
we developed specific memory plan and operator format for our backend, for MAC unit, we convert tvm relay IR to NVDLA compiler's network, for riscv
unit, we transport tvm relay operators' information to manually implemented operators library.

# Compiler workflow  

* Model import: The compiler front-end , TensorFlow, reads the model and converts it into a relay function.  
* Computational graph conversion and optimization: use the optimize function to convert and encapsulate the function, and pass the entire function to the custom codegen.  
* Function division: codegen first divides the function into multiple subfunctions executed on DLA and CPU, and then further divides the subfunction executed by DLA, and converts the multi-output (output as a tuple) DLA function into a single-output DLA function.  
* Function traversal: traverse the subfunctions after the division is completed to generate dependencies between subfunctions.  
* Encapsulate and compile: According to the back-end architecture, convert each subfunction into the final compilation result, and finally encapsulate all the data generated by compilation into the TIHU Module for return.  

# Runtime workflow

* Create a run-time, and get a deployable model from the compiler. The deployable model is a series of computing tasks generated by compiling the AI model based on the architecture of the underlying hardware. The deployable model includes task types, dependencies between tasks, and task parameters, as well as the operator types, dependencies between operators, and operator parameters within tasks.  
* Load models and pictures, parse the models and assign addresses to each task, and call the driver to write parameters such as task data, model data, weights, and pictures into the firmware memory space.  
* Trigger a firmware interrupt, initiate hardware inference, and wait for a hardware interrupt.  
* After the firmware's task is completed and an interrupt is generated, call the driver to obtain the hardware inference result.   

# What will be done

Right now, our backend can't cooperate with other backends, we will make some changes in runtime to add this feature. Next we will explore other codegen strategy
for riscv unit, since it is a general computing device, we may use tvm's llvm codegen and auto tune to get better performance.

License
-------
Â© Contributors Licensed under an [Apache-2.0](LICENSE) license.
